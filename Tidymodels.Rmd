---
title: "Tidymodels"
output: html_document
---

## Intro

I will use 'Credit', a small dataframe from ISLR data package, since the main goal is to explore the predict models only, not data wrangling.

Our goal is to predict the variable "Balance". I will use the mean squared error(MSE) as quality measure for our models.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages

```{r echo=TRUE, message=FALSE, warning=FALSE}

library(tidymodels)
library(ISLR)
library(tidyverse)
library(vip)
library(doParallel)
library(skimr)

```


## Gathering data

```{r}

df <- Credit %>% as_tibble()

```


## Exploring data

```{r}

skim(df)

```

Overall, this data frame is very clean and straightforward for modeling, but the column "ID" shall be removed, since it have no relevant predictive information.

#Removing ID

```{r}

df = subset(df, select = -c(ID))

```

#Rsample

Using Rsample package to set train/test data

```{r}

set.seed(150)

split <- initial_split(df, prop = 0.8) #80% train/20% test

training <- training(split) 
test <- testing(split) 


```

## Using recipe package for data processing


```{r}

recipe1 <- recipe(Balance ~ ., data = training) %>% 

  step_normalize(all_numeric(), -all_outcomes()) %>% # normalizing numeric variables except 'Balance'
  
  step_other(Ethnicity, threshold = .30, other = "Other") %>%  # creating 'Other' category for ethinicy
  
  step_dummy(all_nominal(), -all_outcomes()) # defining all string variables as dummies

summary(recipe1) # display variable roles

(recipe1_prep <- prep(recipe1)) # preparing the recipe

training_proc <- bake(recipe1_prep, new_data = NULL) # processed training dataframe

test_proc <- bake(recipe1_prep, new_data = test) # processed test dataframe


```

## Going for the models using parsnip

Now that we have our training and test dataframe, we will create linear and random forest models

```{r}

lm <- linear_reg() %>% set_engine("lm") # defining linear regression as engine


lm_fit <- linear_reg() %>% set_engine("lm") %>%  fit(Balance ~ ., training_proc)

tidy(lm_fit)


fitted_lm <- lm_fit %>% 
  predict(new_data = test_proc) %>% mutate(observed = test_proc$Balance, model = "lm")

```

## Scatterplot predicted x observed

```{r}

fitted_lm %>% 
  ggplot(aes(observed, .pred)) + #eixo x observado, eixo y predito 
  geom_point(size = 1.5, col = "blue") + 
  labs(y = "Predicted", x = "Observed")

```

Now the random forest

```{r}

rf <- rand_forest() %>% 
  set_engine("ranger", # setting ranger package as engine
           importance = "permutation") %>%  #variable permutation for importance calculation in each tree node
  set_mode("regression")

rf_fit <- rf %>% fit(Balance ~ ., training_proc)
rf_fit

```


Using vip package to check variable importance


```{r}

vip(rf_fit)

```
## testing the random forest

```{r}

fitted_rf <- rf_fit %>% 
  predict(new_data = test_proc) %>% # prediction using test sample
  mutate(observed = test_proc$Balance, # adding observed and model columns to fitted_rf results
         model = "random forest")

```


## Piling fitted_rf and fitted_lm for comparison

```{r}

fitted <- fitted_lm %>% 
  bind_rows(fitted_rf)

```


```{r}

fitted %>% 
  group_by(model) %>% # agrupa pelo modelo ajustado
  metrics(truth = observed, estimate = .pred)

```

## Hyperparameters adjustment using package tune

```{r}

rf2 <- rand_forest(mtry = tune(), # tuning mtry 
                   trees = tune(),  #tuning the number of trees
                   min_n = tune()) %>% #tuning the minimal number of points in a node
  set_engine("ranger") %>% 
  set_mode("regression") 
rf2

# cross validation
set.seed(123)
cv_split <- vfold_cv(training, v = 10)

registerDoParallel() # parallel the processing to make it faster

# assembling the tunning grid
rf_grid <- tune_grid(rf2, 
                     recipe1, # the recipe in each fold
                     resamples = cv_split, # setting folds
                     grid = 10, 
                     metrics = metric_set(rmse, mae)) #setting metrics

autoplot(rf_grid) # plotting results

rf_grid %>% 
  collect_metrics() 

rf_grid %>% 
  select_best("rmse") # select the best combination for the forest hyperparameters

best <- rf_grid %>% 
  select_best("rmse") 


```

## Fiting the random forest with the best hyperparameters 

We will use the finalize_model function that atuomatically uses the hyperparameters setup set as "best" to update our forest


```{r}

rf_fit2 <- finalize_model(rf2, parameters = best) %>% #setting "best" as our hyperparameters optimal combination
  fit(Balance ~ ., training_proc) 

fitted_rf2 <- rf_fit2 %>% 
  predict(new_data = test_proc) %>% 
  mutate(observed = test_proc$Balance, 
         model = "random forest - tune")

```

## models results

```{r}

fitted <- fitted %>% # empilha as previsoes da floresta tunada
  bind_rows(fitted_rf2)

fitted %>% # obtem as metricas de todos os modelos ajustados
  group_by(model) %>% 
  metrics(truth = observed, estimate = .pred) 

```

## Conclusion

As we can see, taking the root of the mean squared error(rmse) as parameter, the linear regression model performed best despite the hyperparameters tuning on the random forest.
